@misc{achiam2023gpt4,
  author = {Achiam, J. and Adler, S. and Agarwal, S. and et al.},
  title = {GPT-4 Technical Report},
  year = {2023},
  eprint = {2303.08774},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  note = {Ithaca, NY: Cornell University Library}
}

@misc{bai2025qwen25vl,
  author = {Bai, S. and Chen, K. and Liu, X. and et al.},
  title = {Qwen2.5-VL Technical Report},
  year = {2025},
  eprint = {2502.13923},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  note = {Ithaca, NY: Cornell University Library}
}

@misc{cai2024temporalbench,
  author = {Cai, M. and Tan, R. and Zhang, J. and et al.},
  title = {TemporalBench: Towards Fine-grained Temporal Understanding for Multimodal Video Models},
  year = {2024},
  note = {OpenReview. \url{https://openreview.net/forum?id=Wto5U7q6I2}. Accessed: 2025-7-10}
}

@misc{chen2021timeqa,
  author = {Chen, W. and Wang, X. and Wang, W. Y.},
  title = {A Dataset for Answering Time-Sensitive Questions},
  year = {2021},
  eprint = {2108.06314},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  note = {Ithaca, NY: Cornell University Library}
}

@misc{chu2023timebench,
  author = {Chu, Z. and Chen, J. and Chen, Q. and et al.},
  title = {TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models},
  year = {2023},
  eprint = {2311.17667},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  note = {Ithaca, NY: Cornell University Library}
}

@misc{cores2024tvbench,
  author = {Cores, D. and Dorkenwald, M. and Mucientes, M. and et al.},
  title = {TVBench: Redesigning Video-Language Evaluation},
  year = {2024},
  note = {OpenReview. \url{https://openreview.net/forum?id=DrNN5qx66Z}. Accessed: 2025-7-10}
}

@article{dhingra2022templama,
  author = {Dhingra, B. and Cole, J. R. and Eisenschlos, J. M. and et al.},
  title = {Time-Aware Language Models as Temporal Knowledge Bases},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {257--273},
  year = {2022}
}

@misc{fatemi2024testoftime,
  author = {Fatemi, B. and Kazemi, M. and Tsitsulin, A. and et al.},
  title = {Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning},
  year = {2024},
  eprint = {2406.09170},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  note = {Ithaca, NY: Cornell University Library}
}

@misc{fu2023mme,
  author = {Fu, C. and Chen, P. and Shen, Y. and et al.},
  title = {MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  year = {2023},
  eprint = {2306.13394},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  note = {Ithaca, NY: Cornell University Library}
}

@misc{imam2025temporalvqa,
  author = {Imam, M. F. and Lyu, C. and Aji, A. F.},
  title = {Can Multimodal LLMs Do Visual Temporal Understanding and Reasoning? The Answer is No!},
  year = {2025},
  eprint = {2501.10674},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  note = {Ithaca, NY: Cornell University Library}
}

@misc{li2024survey,
  author = {Li, J. and Lu, W. and Fei, H. and et al.},
  title = {A Survey on Benchmarks of Multimodal Large Language Models},
  year = {2024},
  eprint = {2408.08632},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  note = {Ithaca, NY: Cornell University Library}
}

@inproceedings{liu2023visualinstruction,
  author = {Liu, H. and Li, C. and Wu, Q. and et al.},
  title = {Visual Instruction Tuning},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {34892--34916},
  year = {2023}
}

@misc{lu2025ovis25,
  author = {Lu, S. and Li, Y. and Xia, Y. and et al.},
  title = {Ovis2.5 Technical Report},
  year = {2025},
  eprint = {2508.11737},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  note = {Ithaca, NY: Cornell University Library}
}

@misc{meem2024patquestions,
  author = {Meem, J. A. and Rashid, M. S. and Dong, Y. and et al.},
  title = {PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering},
  year = {2024},
  eprint = {2402.11034},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  note = {Ithaca, NY: Cornell University Library}
}

@misc{shangguan2024tomato,
  author = {Shangguan, Z. and Li, C. and Ding, Y. and et al.},
  title = {TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models},
  year = {2024},
  eprint = {2410.23266},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  note = {Ithaca, NY: Cornell University Library}
}

@misc{tan2023tempreason,
  author = {Tan, Q. and Ng, H. T. and Bing, L.},
  title = {Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models},
  year = {2023},
  eprint = {2306.08952},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  note = {Ithaca, NY: Cornell University Library}
}

@misc{wang2023tram,
  author = {Wang, Y. and Zhao, Y.},
  title = {TRAM: Benchmarking Temporal Reasoning for Large Language Models},
  year = {2023},
  eprint = {2310.00835},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  note = {Ithaca, NY: Cornell University Library}
}

@misc{wang2025internvl35,
  author = {Wang, W. and Gao, Z. and Gu, L. and et al.},
  title = {InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency},
  year = {2025},
  eprint = {2508.18265},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  note = {Ithaca, NY: Cornell University Library}
}

@misc{yang2025qwen3,
  author = {Yang, A. and Li, A. and Yang, B. and et al.},
  title = {Qwen3 Technical Report},
  year = {2025},
  eprint = {2505.09388},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  note = {Ithaca, NY: Cornell University Library}
}

@article{yin2024survey,
  author = {Yin, S. and Fu, C. and Zhao, S. and et al.},
  title = {A Survey on Multimodal Large Language Models},
  journal = {National Science Review},
  volume = {11},
  number = {12},
  pages = {nwae403},
  year = {2024}
}

@misc{yu2025minicpmv45,
  author = {Yu, T. and Wang, Z. and Wang, C. and et al.},
  title = {MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe},
  year = {2025},
  eprint = {2509.18154},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  note = {Ithaca, NY: Cornell University Library}
}

@inproceedings{yue2024mmmu,
  author = {Yue, X. and Ni, Y. and Zhang, K. and et al.},
  title = {MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {9556--9567},
  year = {2024}
}

@misc{yue2025mimovl,
  author = {Yue, Z. and Lin, Z. and Song, Y. and et al.},
  title = {MiMo-VL Technical Report},
  year = {2025},
  eprint = {2506.03569},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  note = {Ithaca, NY: Cornell University Library}
}
